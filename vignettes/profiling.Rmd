---
title: "Profiling Stan programs with CmdStanR"
author: "Rok Češnovar, Jonah Gabry and Ben Bales"
output: 
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 4
params:
  EVAL: !r identical(Sys.getenv("NOT_CRAN"), "true")
vignette: >
  %\VignetteIndexEntry{Profiling Stan programs with CmdStanR}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r child="children/settings-knitr.Rmd"}
```

## Introduction

The intention of this vignette is to showcase profiling with CmdStanR introduced
in CmdStan 2.26.0.

Profiling is useful for identifying what parts of a model are taking the
longest time to run to guide optimization efforts.

Be aware that the statistical assumptions that go into a model are the
most important factors in overall model performance. It is often not possible
to make up for model problems with just brute force computation. For ideas on
how to address performance of your model from a statistical perspective,
see: [CITE WORKFLOW PAPER].

## Adding profiling statements to a Stan program

Consider a simple logistic regression with parameters `alpha` and `beta`,
covariates `X`, and output `y`.

```stan
data {
  int<lower=1> k;
  int<lower=0> n;
  matrix[n, k] X;
  int y[n];
}
parameters {
  vector[k] beta;
  real alpha;
}
model {
  beta ~ std_normal();
  alpha ~ std_normal();

  y ~ bernoulli_logit(X * beta + alpha);
}
```

A simple question is how much time do the prior calculations take compared
against the likelihood? To answer this, surround the prior and likelihood
calculations with `profile` statements as follows:

```stan
profile("priors") {
  target += std_normal_lpdf(beta);
  target += std_normal_lpdf(alpha);
}
profile("likelihood") {
  target += bernoulli_logit_lpmf(y | X * beta + alpha);
}
```

The model is run as usual, Stan collects the profiling information for any
model with `profile` statements.

```{r, message=FALSE, results='hide'}
library(cmdstanr)

# Compile the model
model <- cmdstan_model("profiling_bernoulli_logit.stan")

# Generate some fake data
n <- 1000
k <- 20
X <- matrix(rnorm(n * k), ncol = k)

y <- 3 * X[,1] - 2 * X[,2] + 1
p <- runif(n)
y <- ifelse(p < (1 / (1 + exp(-y))), 1, 0)
mdata = list(k = ncol(X), n = nrow(X), y = y, X = X)

# Run one chain of the model
fit <- model$sample(data = mdata, chains = 1)
```

The raw profiling information can then be accessed with the `$profiles()`
function, which returns one data frame per chain run (profiles across multiple
chains are not automatically aggregated). Details on the column names are
available in the [CmdStan documentation](https://mc-stan.org/docs/2_26/cmdstan-guide/stan-csv.html#profiling-csv-output-file).

```{r profiles}
fit$profiles()
```

Look at the `total_time` column for the total time spent inside a given
profile statement. It is clear that the vast majority of time is spent
in the likelihood function.

The glm functions can be used to make models like this faster. In this case the
likelihood can be replaced with:

```stan
target += bernoulli_logit_glm_lpmf(y | X, alpha, beta);
```

The profiling information for the new model is collected automatically just
like the last one:

```{r, message=FALSE, results='hide'}
model_glm <- cmdstan_model("profiling_bernoulli_logit_glm.stan")
fit_glm <- model_glm$sample(data = mdata, chains = 1)
```

```{r}
fit_glm$profiles()
```

Looking at the `total_time` column, this is much faster.

## Per-gradient timings, and memory usage

The other columns of the profiling output are documented in the
[CmdStan documentation](https://mc-stan.org/docs/2_26/cmdstan-guide/stan-csv.html#profiling-csv-output-file).

The timing numbers are broken down by forward pass and reverse pass, and
the `chain_stack` and `no_chain_stack` contain information about how many
autodiff variables were saved in the process of performing a calculation.

These are all total numbers -- times are the total times over the whole
calculation, and `chain_stack` counts are similarly the total counts of autodiff
variables use over the whole calculation. It is often convenient to have
per-gradient calculations (which will be more stable across runs with different
seeds). To compute these, use the `autodiff_calls` column.

```{r per-gradient}
profile_chain_1 <- fit$profiles()[[1]]
per_gradient_timing = profile_chain_1["total_time"]/profile_chain_1["autodiff_calls"]
print(per_gradient_timing)
```

## Accessing the profile files

After sampling (or optimization/variational inference) finishes, CmdStan stores
the profiling data in CSV files that are stored in a temporary location. 
The paths of the profiling CSV files can be retrieved using `$profile_files()`.

```{r profile_files}
fit$profile_files()
```

These can be saved to a more permanent location with the `$save_profile_files()`
method.

```{r save_profile_files, eval=FALSE}
# see ?save_profile_files for info on optional arguments
fit$save_profile_files(dir = "path/to/directory")
```
